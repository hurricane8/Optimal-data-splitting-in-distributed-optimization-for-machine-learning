@article{kovalev2022optimal,
  title={Optimal gradient sliding and its application to optimal distributed optimization under similarity},
  author={Kovalev, Dmitry and Beznosikov, Aleksandr and Borodich, Ekaterina and Gasnikov, Alexander and Scutari, Gesualdo},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={33494--33507},
  year={2022}
}

@article{kim2021optimizing,
  title={Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions},
  author={Kim, Donghwan and Fessler, Jeffrey A},
  journal={Journal of optimization theory and applications},
  volume={188},
  number={1},
  pages={192--219},
  year={2021},
  publisher={Springer}
}

@article{arjevani2015communication,
  title={Communication complexity of distributed convex learning and optimization},
  author={Arjevani, Yossi and Shamir, Ohad},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{shamir2014communication,
  title={Communication-efficient distributed optimization using an approximate newton-type method},
  author={Shamir, Ohad and Srebro, Nati and Zhang, Tong},
  booktitle={International conference on machine learning},
  pages={1000--1008},
  year={2014},
  organization={PMLR}
}

@article{matsushima2014distributed,
  title={Distributed stochastic optimization of the regularized risk},
  author={Matsushima, Shin and Yun, Hyokun and Zhang, Xinhua and Vishwanathan, SVN},
  journal={arXiv preprint arXiv:1406.4363},
  year={2014}
}

@inproceedings{tian2022acceleration,
  title={Acceleration in distributed optimization under similarity},
  author={Tian, Ye and Scutari, Gesualdo and Cao, Tianyu and Gasnikov, Alexander},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5721--5756},
  year={2022},
  organization={PMLR}
}

@article{sun2022distributed,
  title={Distributed optimization based on gradient tracking revisited: Enhancing convergence rate via surrogation},
  author={Sun, Ying and Scutari, Gesualdo and Daneshmand, Amir},
  journal={SIAM Journal on Optimization},
  volume={32},
  number={2},
  pages={354--385},
  year={2022},
  publisher={SIAM}
}

@article{reddi2016aide,
  title={Aide: Fast and communication efficient distributed optimization},
  author={Reddi, Sashank J and Kone{\v{c}}n{\`y}, Jakub and Richt{\'a}rik, Peter and P{\'o}cz{\'o}s, Barnab{\'a}s and Smola, Alex},
  journal={arXiv preprint arXiv:1608.06879},
  year={2016}
}

@inproceedings{hendrikx2020statistically,
  title={Statistically preconditioned accelerated gradient method for distributed optimization},
  author={Hendrikx, Hadrien and Xiao, Lin and Bubeck, Sebastien and Bach, Francis and Massoulie, Laurent},
  booktitle={International conference on machine learning},
  pages={4203--4227},
  year={2020},
  organization={PMLR}
}

@article{beznosikov2021distributed,
  title={Distributed saddle-point problems under data similarity},
  author={Beznosikov, Aleksandr and Scutari, Gesualdo and Rogozin, Alexander and Gasnikov, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8172--8184},
  year={2021}
}

@article{verbraeken2020survey,
  title={A survey on distributed machine learning},
  author={Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S},
  journal={Acm computing surveys (csur)},
  volume={53},
  number={2},
  pages={1--33},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@inproceedings{ghosh2020communication,
  title={Communication efficient distributed approximate Newton method},
  author={Ghosh, Avishek and Maity, Raj Kumar and Mazumdar, Arya and Ramchandran, Kannan},
  booktitle={2020 IEEE International Symposium on Information Theory (ISIT)},
  pages={2539--2544},
  year={2020},
  organization={IEEE}
}

@article{smith2018cocoa,
  title={CoCoA: A general framework for communication-efficient distributed optimization},
  author={Smith, Virginia and Forte, Simone and Chenxin, Ma and Tak{\'a}{\v{c}}, Martin and Jordan, Michael I and Jaggi, Martin},
  journal={Journal of Machine Learning Research},
  volume={18},
  pages={230},
  year={2018},
  publisher={MIT press}
}

@inproceedings{gorbunov2021marina,
  title={MARINA: Faster non-convex distributed learning with compression},
  author={Gorbunov, Eduard and Burlachenko, Konstantin P and Li, Zhize and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={3788--3798},
  year={2021},
  organization={PMLR}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}

@article{polyak2007newton,
  title={Newtonâ€™s method and its use in optimization},
  author={Polyak, Boris T},
  journal={European Journal of Operational Research},
  volume={181},
  number={3},
  pages={1086--1096},
  year={2007},
  publisher={Elsevier}
}

