@article{kovalev2022optimal,
  title={Optimal gradient sliding and its application to optimal distributed optimization under similarity},
  author={Kovalev, Dmitry and Beznosikov, Aleksandr and Borodich, Ekaterina and Gasnikov, Alexander and Scutari, Gesualdo},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={33494--33507},
  year={2022}
}

@article{kim2021optimizing,
  title={Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions},
  author={Kim, Donghwan and Fessler, Jeffrey A},
  journal={Journal of optimization theory and applications},
  volume={188},
  number={1},
  pages={192--219},
  year={2021},
  publisher={Springer}
}

@article{arjevani2015communication,
  title={Communication complexity of distributed convex learning and optimization},
  author={Arjevani, Yossi and Shamir, Ohad},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{shamir2014communication,
  title={Communication-efficient distributed optimization using an approximate newton-type method},
  author={Shamir, Ohad and Srebro, Nati and Zhang, Tong},
  booktitle={International conference on machine learning},
  pages={1000--1008},
  year={2014},
  organization={PMLR}
}

@article{matsushima2014distributed,
  title={Distributed stochastic optimization of the regularized risk},
  author={Matsushima, Shin and Yun, Hyokun and Zhang, Xinhua and Vishwanathan, SVN},
  journal={arXiv preprint arXiv:1406.4363},
  year={2014}
}

@inproceedings{tian2022acceleration,
  title={Acceleration in distributed optimization under similarity},
  author={Tian, Ye and Scutari, Gesualdo and Cao, Tianyu and Gasnikov, Alexander},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5721--5756},
  year={2022},
  organization={PMLR}
}

@article{sun2022distributed,
  title={Distributed optimization based on gradient tracking revisited: Enhancing convergence rate via surrogation},
  author={Sun, Ying and Scutari, Gesualdo and Daneshmand, Amir},
  journal={SIAM Journal on Optimization},
  volume={32},
  number={2},
  pages={354--385},
  year={2022},
  publisher={SIAM}
}

@article{reddi2016aide,
  title={Aide: Fast and communication efficient distributed optimization},
  author={Reddi, Sashank J and Kone{\v{c}}n{\`y}, Jakub and Richt{\'a}rik, Peter and P{\'o}cz{\'o}s, Barnab{\'a}s and Smola, Alex},
  journal={arXiv preprint arXiv:1608.06879},
  year={2016}
}

@inproceedings{hendrikx2020statistically,
  title={Statistically preconditioned accelerated gradient method for distributed optimization},
  author={Hendrikx, Hadrien and Xiao, Lin and Bubeck, Sebastien and Bach, Francis and Massoulie, Laurent},
  booktitle={International conference on machine learning},
  pages={4203--4227},
  year={2020},
  organization={PMLR}
}

@article{beznosikov2021distributed,
  title={Distributed saddle-point problems under data similarity},
  author={Beznosikov, Aleksandr and Scutari, Gesualdo and Rogozin, Alexander and Gasnikov, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8172--8184},
  year={2021}
}